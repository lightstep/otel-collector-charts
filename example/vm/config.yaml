# Last Collector-Contrib Validation: v0.96.0
receivers:
    prometheus/self:
        config:
            scrape_configs:
              - job_name: otel-collector
                scrape_interval: 5s
                static_configs:
                - labels:
                    collector_name: sn-collector
                  targets:
                    - 0.0.0.0:8888
    hostmetrics:
    # Hostmetrics scrapes metrics from various host systems.
    # For more details, see
    # https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/hostmetricsreceiver/README.md
        collection_interval: "30s"
        root_path: /hostfs
        scrapers:
            cpu:
                metrics:
                    system.cpu.utilization:
                        enabled: true
            disk: {}
            # cpu and disk are only supported if build with CGO_ENABLED=1
            load: {}
            filesystem:
                metrics:
                    system.filesystem.utilization:
                        enabled: true
                exclude_mount_points:
                    match_type: regexp
                    mount_points:
                        - /dev/.*
                        - /proc/.*
                        - /sys/.*
                        - /run/k3s/containerd/.*
                        - /var/lib/docker/.*
                        - /var/lib/kubelet/.*
                        - /snap/.*
                exclude_fs_types:
                    match_type: strict
                    fs_types:
                        - autofs
                        - binfmt_misc
                        - bpf
                        - cgroup2
                        - configfs
                        - debugfs
                        - devpts
                        - devtmpfs
                        - fusectl
                        - hugetlbfs
                        - iso9660
                        - mqueue
                        - nsfs
                        - overlay
                        - proc
                        - procfs
                        - pstore
                        - rpc_pipefs
                        - securityfs
                        - selinuxfs
                        - squashfs
                        - sysfs
                        - tracefs
            memory:
                metrics:
                    system.memory.utilization:
                        enabled: true
            # paging:
            # processes:
            # process:
            network: {}
    otlp:
    # OTLP is the default protocol supported by all language implementations.
    # For more details, see
    # https://github.com/open-telemetry/opentelemetry-collector/blob/main/receiver/otlpreceiver/README.md
        protocols:
            grpc:
                endpoint:
            http:
                endpoint:
processors:
    # We recommend use of the batch processor.  We recommend the settings
    # below for traces.
    #
    # Note: We are aware of ongoing efforts within OpenTelemetry to
    # configure batching in the exporter, where it is possible to
    # configure batch size limits in terms of bytes, instead of items.
    # We will update these recommendations when batching by size is
    # available.
    batch:
        # In this example, the processor will wait to accumulate at least
        # 1000 spans for up to 1 second, then flush the batch.  In cases
        # where the arriving data is already batched, such that combining
        # the pending batch with the arriving data would exceed 1500
        # items, then 1500 items will be sent by splitting the data.
        #
        # Note: the batch processor has a side-effect of returning success
        # to the producer, before waiting for the consumer to respond.
        # This is appropriate default in most cases, it means that SDKs
        # sending to the gateway will not see or report errors.
        #
        # The batch processor responds to "back-pressure" from the
        # exporter, meaning it is never directly responsible for dropping
        # spans.  Note that our current recommendation for exporter
        # settings does not respond with back-pressure to the batch
        # processor.  Due to exporter settings, this collector
        # configuration will drop data when the ServiceNow service is
        # (intentionally or accidentally) refusing data, instead of
        # applying pressure backward, discussed in the `exporters`
        # section.
        send_batch_size: 1000
        timeout: 1s
        send_batch_max_size: 1500

exporters:
    otlp/ls:
        endpoint: ingest.lightstep.com:443 # US data center
        # endpoint: ingest.eu.lightstep.com:443 # EU data center
        headers:
            lightstep-access-token: "${LIGHTSTEP_ACCESS_TOKEN}"
        # While we expect latency under one second, typically, we
        # recommend a longer timeout than the default.
        timeout: 5s
        # Queue settings are required.  It does not make sense to use
        # the exporter without a queue, it has to do with
        # requiring the "num_consumers" limit configured in this
        # section (i.e., we require a queue in order to limit the
        # number of concurrent exports).
        #
        # Note that the queue settings are applied in unit-terms
        # produced by the batch processor, so a number like 100 means
        # the queue has support for 100 pre-batched items.  With up to
        # 1500 spans each (from the batch processor), this
        # configuration allows 150,000 spans to occupy memory.
        sending_queue:
            enabled: true
            num_consumers: 4
            queue_size: 100
        # Retry settings are optional.
        #
        # Note that while retries are attempted, this component will
        # begin to drop arriving data if the queue is not large
        # enough.
        retry_on_failure:
            # We recommend disabling retries, since while the export is
            # blocked it is likely that arriving spans will drop, and
            # Otherwise, collectors will need substantial additional
            # memory to survive transient failures.  Nevertheless, we
            # recommend a limited retry policy to gracefully occasional
            # failures, paired with a modest queue size.
            #
            # Note there is a persistent storage option inherited from a
            # common collector component.  When persistent storage is
            # configured, the default retry configuration is sensible.
            #
            # For more details on retry and queue settings, please refer to
            # https://github.com/open-telemetry/opentelemetry-collector/blob/main/exporter/exporterhelper/README.md
            enabled: true
            max_elapsed_time: 30s
    debug:
        verbosity: basic
        sampling_initial: 5
        sampling_thereafter: 200

extensions:
    # The healthcheck extension provides an endpoint that provides
    # support to run liveness checks for the Collector.
    # For more details, see
    # https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/healthcheckextension/README.md
    health_check:
    # If this collector is not running in Kubernetes, we recommend adding
    # this opampextension to report its config and status.
    # For more details, see https://docs.lightstep.com/docs/monitor-collector-health
    opamp:
        server:
            ws:
                endpoint: "wss://opamp.lightstep.com/v1/opamp"
                headers:
                    "Authorization": "bearer ${LS_OPAMP_API_KEY}"

service:
    extensions: [health_check, opamp]
    telemetry:
        metrics:
          address: :8888
    pipelines:
        traces:
            receivers: [otlp]
            processors: [batch]
            exporters: [debug, otlp/ls]
        metrics:
            receivers: [otlp, prometheus/self, hostmetrics]
            processors: [batch]
            exporters: [debug, otlp/ls]
        logs:
            exporters: [debug, otlp/ls]
            processors: [batch]
            receivers: [otlp] # update with your receiver name