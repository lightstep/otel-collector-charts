extraEnvs: []
debug: false
clusterName: "unknown"
otlpDestinationOverride: ""

## Auto-Instrumentation resource to be installed in the cluster
## Can be used by setting the following:
##  Java: instrumentation.opentelemetry.io/inject-java: "true"
##  NodeJS: instrumentation.opentelemetry.io/inject-nodejs: "true"
##  Python: instrumentation.opentelemetry.io/inject-python: "true"
##  DotNet: instrumentation.opentelemetry.io/inject-dotnet: "true"
##  OpenTelemetry SDK environment variables only: instrumentation.opentelemetry.io/inject-sdk: "true"
autoinstrumentation:
  enabled: false
  ## The collector name to send traces to
  collectorTarget: traces
  propagators:
    - tracecontext
    - baggage
    - b3

  ## Sampler defines the OTEL sampler behavior to be used. Example:
  ##
  ## sampler:
  ##   type: parentbased_traceidratio
  ##   argument: "0.25"
  ##
  sampler:
    ## The value can be for instance parentbased_always_on, parentbased_always_off, parentbased_traceidratio...
    type: parentbased_traceidratio
    ## The value depends on the sampler type.
    ## For instance for parentbased_traceidratio sampler type it is a number in range [0..1] e.g. 0.25.
    argument: "0.25"

  ## A list of corev1.EnvVars
  env: []

  ## https://github.com/open-telemetry/opentelemetry-specification/blob/v1.8.0/specification/overview.md#resources
  resource: {}

daemonCollector:
  name: daemon
  clusterName: ""
  image:
    repository: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib
    tag: "0.96.0"
  enabled: true
  mode: daemonset
  mountHostFS: true
  env:
    - name: LS_TOKEN
      valueFrom:
        secretKeyRef:
          key: LS_TOKEN
          name: otel-collector-secret
  resources:
    limits:
      cpu: 100m
      memory: 250Mi
    requests:
      cpu: 100m
      memory: 128Mi
  scrape_configs_file: "daemon_scrape_configs.yaml"
  config:
    receivers:
      # OTel-Arrow notes: For this collector to receive OTel-Arrow in
      # addition to standard forms of OTLP, use an image with the
      # OTel-Arrow receiver component replace "otlp" below with
      # "otelarrow".
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
      kubeletstats:
        collection_interval: "15s"
        auth_type: "serviceAccount"
        insecure_skip_verify: true
        # For this scrape to work, the RBAC must have `nodes/stats` GET access.
        endpoint: "https://${env:K8S_NODE_IP}:10250"
        extra_metadata_labels:
          - container.id
          - k8s.volume.type
        metric_groups:
          - node
          - pod
          - volume
          - container
        k8s_api_config:
          auth_type: serviceAccount
      hostmetrics:
        collection_interval: "30s"
        root_path: /hostfs
        scrapers:
          cpu:
            metrics:
              system.cpu.utilization:
                enabled: true
          disk: {}
          load: {}
          filesystem:
            metrics:
              system.filesystem.utilization:
                enabled: true
            exclude_mount_points:
              match_type: regexp
              mount_points:
                - /dev/.*
                - /proc/.*
                - /sys/.*
                - /run/k3s/containerd/.*
                - /var/lib/docker/.*
                - /var/lib/kubelet/.*
                - /snap/.*
            exclude_fs_types:
              match_type: strict
              fs_types:
                - autofs
                - binfmt_misc
                - bpf
                - cgroup2
                - configfs
                - debugfs
                - devpts
                - devtmpfs
                - fusectl
                - hugetlbfs
                - iso9660
                - mqueue
                - nsfs
                - overlay
                - proc
                - procfs
                - pstore
                - rpc_pipefs
                - securityfs
                - selinuxfs
                - squashfs
                - sysfs
                - tracefs
          memory:
            metrics:
              system.memory.utilization:
                enabled: true
          # paging:
          # processes:
          # process:
          network: {}
    processors:
      resourcedetection/env:
        detectors: [env]
        timeout: 2s
        override: false
      k8sattributes:
        passthrough: false
        filter:
          node_from_env_var: K8S_NODE_NAME
        pod_association:
          # Pod assocations are used to extract a pod ID from one or more sources.
          # These are used to match up telemetry.
          # Each has a maximum of 4 association sources.
          - sources:
              - from: resource_attribute
                name: k8s.pod.uid
          - sources:
              - from: resource_attribute
                name: k8s.pod.name
              - from: resource_attribute
                name: k8s.namespace.name
              - from: resource_attribute
                name: k8s.node.name
          - sources:
              - from: resource_attribute
                name: k8s.pod.ip
          - sources:
              - from: resource_attribute
                name: k8s.pod.name
              - from: resource_attribute
                name: k8s.namespace.name
          - sources:
              - from: connection
        extract:
          labels:
            - tag_name: service.name
              key: app.kubernetes.io/name
              from: pod
            - tag_name: service.name
              key: k8s-app
              from: pod
            - tag_name: k8s.app.instance
              key: app.kubernetes.io/instance
              from: pod
            - tag_name: service.version
              key: app.kubernetes.io/version
              from: pod
            - tag_name: k8s.app.component
              key: app.kubernetes.io/component
              from: pod
          metadata:
            - k8s.namespace.name
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.node.name
            - k8s.pod.start_time
            - k8s.deployment.name
            - k8s.replicaset.name
            - k8s.replicaset.uid
            - k8s.daemonset.name
            - k8s.daemonset.uid
            - k8s.job.name
            - k8s.job.uid
            - k8s.container.name
            - k8s.cronjob.name
            - k8s.statefulset.name
            - k8s.statefulset.uid
            - container.image.tag
            - container.image.name
            - k8s.cluster.uid
      # We recommend use of the batch processor.  We recommend the settings
      # below for traces.
      #
      # Note: We are aware of ongoing efforts within OpenTelemetry to
      # configure batching in the exporter, where it is possible to
      # configure batch size limits in terms of bytes, instead of items.
      # We will update these recommendations when batching by size is
      # available.
      batch:
        # In this example, the processor will wait to accumulate at least
        # 1000 spans for up to 1 second, then flush the batch.  In cases
        # where the arriving data is already batched, such that combining
        # the pending batch with the arriving data would exceed 1500
        # items, then 1500 items will be sent by splitting the data.
        #
        # Note: the batch processor has a side-effect of returning success
        # to the producer, before waiting for the consumer to respond.
        # This is appropriate default in most cases, it means that SDKs
        # sending to the gateway will not see or report errors.
        #
        # The batch processor responds to "back-pressure" from the
        # exporter, meaning it is never directly responsible for dropping
        # spans.  Note that our current recommendation for exporter
        # settings does not respond with back-pressure to the batch
        # processor.  Due to exporter settings, this collector
        # configuration will drop data when the ServiceNow service is
        # (intentionally or accidentally) refusing data, instead of
        # applying pressure backward, discussed in the `exporters`
        # section.
        send_batch_size: 1000
        send_batch_max_size: 1500
        timeout: 1s
    exporters:
      logging:
        verbosity: detailed
        sampling_initial: 5
        sampling_thereafter: 200
      otlp:
        endpoint: ingest.lightstep.com:443
        headers:
          "lightstep-access-token": "${LS_TOKEN}"
        sending_queue:
          enabled: true
          num_consumers: 4
          queue_size: 100

        # Retry settings are optional.
        #
        # Note that while retries are attempted, this component will
        # begin to drop arriving data if the queue is not large
        # enough.
        retry_on_failure:
          # We recommend disabling retries, since while the export is
          # blocked it is likely that arriving spans will drop, and
          # Otherwise, collectors will need substantial additional
          # memory to survive transient failures.  Nevertheless, we
          # recommend a limited retry policy to gracefully occasional
          # failures, paired with a modest queue size.
          #
          # Note there is a persistent storage option inherited from a
          # common collector component.  When persistent storage is
          # configured, the default retry configuration is sensible.
          #
          # For more details on retry and queue settings, please refer to
          # https://github.com/open-telemetry/opentelemetry-collector/blob/main/exporter/exporterhelper/README.md
          enabled: true
          max_elapsed_time: 60s

        # While we expect latency under one second, typically, we
        # recommend a longer timeout than the default.
        timeout: 30s
    service:
      pipelines:
        metrics:
          exporters:
            - otlp
          processors:
            - resourcedetection/env
            - k8sattributes
            - batch
          receivers:
            - prometheus/file
            - otlp
            - kubeletstats
            - hostmetrics

# This is a singleton collector for cluster-level data
clusterCollector:
  name: cluster-stats
  clusterName: ""
  image: 
    repository: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib
    tag: "0.96.0"
  replicas: 1
  mode: deployment
  enabled: true
  mountHostFS: false
  resources:
    limits:
      cpu: 100m
      memory: 500Mi
    requests:
      cpu: 100m
      memory: 500Mi
  env:
    - name: LS_TOKEN
      valueFrom:
        secretKeyRef:
          key: LS_TOKEN
          name: otel-collector-secret
  config:
    receivers:
      k8s_cluster:
        auth_type: serviceAccount
        collection_interval: 10s
        node_conditions_to_report:
          [Ready, MemoryPressure, DiskPressure, NetworkUnavailable]
        allocatable_types_to_report: [cpu, memory, storage]
      k8s_events:
        auth_type: serviceAccount

    processors:
      k8sattributes:
        passthrough: false
        pod_association:
          - sources:
              - from: resource_attribute
                name: k8s.pod.uid
          - sources:
              - from: resource_attribute
                name: k8s.pod.name
              - from: resource_attribute
                name: k8s.namespace.name
              - from: resource_attribute
                name: k8s.node.name
          - sources:
              - from: resource_attribute
                name: k8s.pod.ip
          - sources:
              - from: resource_attribute
                name: k8s.pod.name
              - from: resource_attribute
                name: k8s.namespace.name
          - sources:
              - from: connection
        extract:
          labels:
            - tag_name: service.name
              key: app.kubernetes.io/name
              from: pod
            - tag_name: service.name
              key: k8s-app
              from: pod
            - tag_name: k8s.app.instance
              key: app.kubernetes.io/instance
              from: pod
            - tag_name: service.version
              key: app.kubernetes.io/version
              from: pod
            - tag_name: k8s.app.component
              key: app.kubernetes.io/component
              from: pod
          metadata:
            - k8s.namespace.name
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.node.name
            - k8s.pod.start_time
            - k8s.deployment.name
            - k8s.replicaset.name
            - k8s.replicaset.uid
            - k8s.daemonset.name
            - k8s.daemonset.uid
            - k8s.job.name
            - k8s.job.uid
            - k8s.container.name
            - k8s.cronjob.name
            - k8s.statefulset.name
            - k8s.statefulset.uid
            - container.image.tag
            - container.image.name
            - k8s.cluster.uid
      resourcedetection/env:
        detectors: [env]
        timeout: 2s
        override: false
      batch:
        send_batch_size: 1000
        timeout: 1s
        send_batch_max_size: 1500

    exporters:
      logging:
        verbosity: normal
        sampling_initial: 5
        sampling_thereafter: 200
      otlp:
        endpoint: ingest.lightstep.com:443
        headers:
          "lightstep-access-token": "${LS_TOKEN}"

    service:
      pipelines:
        metrics/k8s_cluster:
          receivers: [k8s_cluster]
          processors: [resourcedetection/env, k8sattributes, batch]
          exporters: [otlp, logging]

## Default collector for tracing
tracesCollector:
  enabled: false
  name: traces
  clusterName: ""
  image: 
    repository: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib
    tag: "0.96.0"
  mode: deployment
  hpa:
    minReplicas: 1
    maxReplicas: 3
    targetMemoryUtilization: 70
  resources:
    limits:
      cpu: 250m
      memory: 250Mi
    requests:
      cpu: 250m
      memory: 250Mi
  env:
    - name: LS_TOKEN
      valueFrom:
        secretKeyRef:
          key: LS_TOKEN
          name: otel-collector-secret
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: "0.0.0.0:4317"
          http:
            endpoint: "0.0.0.0:4318"
    processors:
      resourcedetection/env:
        detectors: [env]
        timeout: 2s
        override: false
      batch:
        send_batch_size: 1000
        timeout: 1s
        send_batch_max_size: 1500
      k8sattributes:
        passthrough: false
        pod_association:
          - sources:
              - from: resource_attribute
                name: k8s.pod.uid
          - sources:
              - from: resource_attribute
                name: k8s.pod.name
              - from: resource_attribute
                name: k8s.namespace.name
              - from: resource_attribute
                name: k8s.node.name
          - sources:
              - from: resource_attribute
                name: k8s.pod.ip
          - sources:
              - from: resource_attribute
                name: k8s.pod.name
              - from: resource_attribute
                name: k8s.namespace.name
          - sources:
              - from: connection
        extract:
          labels:
            - tag_name: service.name
              key: app.kubernetes.io/name
              from: pod
            - tag_name: service.name
              key: k8s-app
              from: pod
            - tag_name: k8s.app.instance
              key: app.kubernetes.io/instance
              from: pod
            - tag_name: service.version
              key: app.kubernetes.io/version
              from: pod
            - tag_name: k8s.app.component
              key: app.kubernetes.io/component
              from: pod
          metadata:
            - k8s.namespace.name
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.node.name
            - k8s.pod.start_time
            - k8s.deployment.name
            - k8s.replicaset.name
            - k8s.replicaset.uid
            - k8s.daemonset.name
            - k8s.daemonset.uid
            - k8s.job.name
            - k8s.job.uid
            - k8s.container.name
            - k8s.cronjob.name
            - k8s.statefulset.name
            - k8s.statefulset.uid
            - container.image.tag
            - container.image.name
            - k8s.cluster.uid

    exporters:
      otlp:
        endpoint: ingest.lightstep.com:443
        headers:
          "lightstep-access-token": "${LS_TOKEN}"
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors:
            - resourcedetection/env
            - k8sattributes
            - batch
          exporters: [otlp]

logsCollector:
  enabled: false
  name: logs
  clusterName: ""
  image: 
    repository: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib
    tag: "0.96.0"
  mode: daemonset
  resources:
    limits:
      cpu: 100m
      memory: 250Mi
    requests:
      cpu: 100m
      memory: 128Mi
  env:
    - name: LS_TOKEN
      valueFrom:
        secretKeyRef:
          key: LS_TOKEN
          name: otel-collector-secret
  volumeMounts:
    - mountPath: /var/log
      name: varlog
      readOnly: true
    - mountPath: /var/lib/docker/containers
      name: varlibdockercontainers
      readOnly: true
  volumes:
    - name: varlog
      hostPath:
        path: /var/log
    - name: varlibdockercontainers
      hostPath:
        path: /var/lib/docker/containers
  config:
    receivers:
      k8s_events: {}
      # inspired by https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/081678933ad0246a0fcb9564c8f1871480a306aa/examples/kubernetes/otel-collector-config.yml
      filelog:
        include:
          - /var/log/pods/*/*/*.log
        start_at: beginning
        include_file_path: true
        include_file_name: false
        operators:
          # Find out which format is used by kubernetes
          - type: router
            id: get-format
            routes:
              - output: parser-docker
                expr: 'body matches "^\\{"'
              - output: parser-crio
                expr: 'body matches "^[^ Z]+ "'
              - output: parser-containerd
                expr: 'body matches "^[^ Z]+Z"'
          # Parse CRI-O format
          - type: regex_parser
            id: parser-crio
            regex: "^(?P<time>[^ Z]+) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$"
            output: extract_metadata_from_filepath
            timestamp:
              parse_from: attributes.time
              layout_type: gotime
              layout: "2006-01-02T15:04:05.999999999Z07:00"
          # Parse CRI-Containerd format
          - type: regex_parser
            id: parser-containerd
            regex: "^(?P<time>[^ ^Z]+Z) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$"
            output: extract_metadata_from_filepath
            timestamp:
              parse_from: attributes.time
              layout: "%Y-%m-%dT%H:%M:%S.%LZ"
          # Parse Docker format
          - type: json_parser
            id: parser-docker
            output: extract_metadata_from_filepath
            timestamp:
              parse_from: attributes.time
              layout: "%Y-%m-%dT%H:%M:%S.%LZ"
          - type: move
            from: attributes.log
            to: body
          # Extract metadata from file path
          - type: regex_parser
            id: extract_metadata_from_filepath
            regex: '^.*\/(?P<namespace>[^_]+)_(?P<pod_name>[^_]+)_(?P<uid>[a-f0-9\-]{36})\/(?P<container_name>[^\._]+)\/(?P<restart_count>\d+)\.log$'
            parse_from: attributes["log.file.path"]
            cache:
              # default maximum amount of Pods per Node is 110
              size: 128
          # Rename attributes
          - type: move
            from: attributes.stream
            to: attributes["log.iostream"]
          - type: move
            from: attributes.container_name
            to: resource["k8s.container.name"]
          - type: move
            from: attributes.namespace
            to: resource["k8s.namespace.name"]
          - type: move
            from: attributes.pod_name
            to: resource["k8s.pod.name"]
          - type: move
            from: attributes.restart_count
            to: resource["k8s.container.restart_count"]
          - type: move
            from: attributes.uid
            to: resource["k8s.pod.uid"]
    processors:
      memory_limiter:
        check_interval: 1s
        limit_percentage: 75
        spike_limit_percentage: 30
      resourcedetection/env:
        detectors: [env]
        timeout: 2s
        override: false
      batch:
        send_batch_size: 1000
        timeout: 1s
        send_batch_max_size: 1500
      k8sattributes:
        passthrough: false
        pod_association:
          - sources:
              - from: resource_attribute
                name: k8s.pod.uid
          - sources:
              - from: resource_attribute
                name: k8s.pod.name
              - from: resource_attribute
                name: k8s.namespace.name
              - from: resource_attribute
                name: k8s.node.name
          - sources:
              - from: resource_attribute
                name: k8s.pod.ip
          - sources:
              - from: resource_attribute
                name: k8s.pod.name
              - from: resource_attribute
                name: k8s.namespace.name
          - sources:
              - from: connection
        extract:
          labels:
            - tag_name: service.name
              key: app.kubernetes.io/name
              from: pod
            - tag_name: service.name
              key: k8s-app
              from: pod
            - tag_name: k8s.app.instance
              key: app.kubernetes.io/instance
              from: pod
            - tag_name: service.version
              key: app.kubernetes.io/version
              from: pod
            - tag_name: k8s.app.component
              key: app.kubernetes.io/component
              from: pod
          metadata:
            - k8s.namespace.name
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.node.name
            - k8s.pod.start_time
            - k8s.deployment.name
            - k8s.replicaset.name
            - k8s.replicaset.uid
            - k8s.daemonset.name
            - k8s.daemonset.uid
            - k8s.job.name
            - k8s.job.uid
            - k8s.container.name
            - k8s.cronjob.name
            - k8s.statefulset.name
            - k8s.statefulset.uid
            - container.image.tag
            - container.image.name
            - k8s.cluster.uid

    exporters:
      otlp:
        endpoint: ingest.lightstep.com:443
        headers:
          "lightstep-access-token": "${LS_TOKEN}"

    service:
      pipelines:
        logs:
          receivers: [k8s_events, filelog]
          processors:
            - memory_limiter
            - resourcedetection/env
            - k8sattributes
            - batch
          exporters: [otlp]

collectors: []

opAMPBridge:
  enabled: true
  # Adds `opentelemetry.io/opamp-reporting: true` to all collectors
  addReportingLabel: true
  # Adds `opentelemetry.io/opamp-managed: true` to all collectors
  addManagedLabel: false
  image:
    repository: ghcr.io/open-telemetry/opentelemetry-operator/operator-opamp-bridge
    tag: "0.96.0"
  endpoint: "wss://opamp.lightstep.com/v1/opamp"
  headers:
    "Authorization": "bearer ${LS_OPAMP_API_KEY}"
  env:
    - name: LS_OPAMP_API_KEY
      valueFrom:
        secretKeyRef:
          key: LS_OPAMP_API_KEY
          name: otel-opamp-bridge-secret
          optional: true
  capabilities:
    AcceptsOpAMPConnectionSettings: true
    AcceptsOtherConnectionSettings: true
    AcceptsRemoteConfig: true
    AcceptsRestartCommand: true
    ReportsEffectiveConfig: true
    ReportsHealth: true
    ReportsOwnLogs: true
    ReportsOwnMetrics: true
    ReportsOwnTraces: true
    ReportsRemoteConfig: true
    ReportsStatus: true
